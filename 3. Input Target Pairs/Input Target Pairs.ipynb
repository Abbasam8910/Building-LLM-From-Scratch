{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6462562e",
   "metadata": {},
   "source": [
    "# Input Target Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cced74e",
   "metadata": {},
   "source": [
    "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732c1169",
   "metadata": {},
   "source": [
    "To get started, we will first tokenize the whole The Verdict short story we worked with earlier using the BPE tokenizer introduced in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d849ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "708cd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee07302f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac97e1c",
   "metadata": {},
   "source": [
    "\n",
    "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it\n",
    "results in a slightly more interesting text passage in the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a61a727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5095\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "print(len(enc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783fb0c",
   "metadata": {},
   "source": [
    "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by 1:\n",
    "\n",
    "\n",
    "The context size determines how many tokens are included in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ec8c8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "Y: [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 # Length of the input\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) \n",
    "#to predict the next word in the sequence. \n",
    "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"Y: {y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff5b9c9",
   "metadata": {},
   "source": [
    "Processing the inputs along with the targets, which are the inputs shifted by one position, we can then create the next-word prediction tasks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd920db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b94b40",
   "metadata": {},
   "source": [
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token ID on the right side of the arrow represents the target token ID that the LLM is supposed to predict.\n",
    "\n",
    "\n",
    "For illustration purposes, let's repeat the previous code but convert the token IDs into text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e16c8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee7e71",
   "metadata": {},
   "source": [
    "We've now created the input-target pairs that we can turn into use for the LLM training in upcoming chapters.\n",
    "\n",
    "There's only one more task before we can turn the tokens into embeddings:implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays.\n",
    "\n",
    "In particular, we are interested in returning two tensors: an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLM to predict,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b74683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
